---
title: "HPLC SeaBass Submission"
author: "Sebastian Di Geronimo"
date: '2023-03-13'
format: html
---
# 1.0 Information
## 1.1 Links: SeaBass Instructions for Submission of HPLC
Instructions: https://seabass.gsfc.nasa.gov/wiki/Data_Submission
Check before submit: https://seabass.gsfc.nasa.gov/wiki/FCHECK#Download%20Source%20Code

## 1.2 Header
https://seabass.gsfc.nasa.gov/wiki/metadataheaders

### 1.2.1 Required Fields:
investigators
affiliations
contact
experiment
cruise
station
data_file_name
documetns
data_type
calibration_files
start_date
end_date
start_time
end_time
north_latitude
south_latitude
east_longitude
west_longitude
water_depth
missing
delimiter
fields
units

### 1.2.2 Conditionally Required Headers


### 1.2.3 HPLC Specific
https://seabass.gsfc.nasa.gov/wiki/data_submission_special_requirements#Pigments,%20HPLC
/HPLC_lab (e.g., NASA_GSFC)
/HPLC_lab_technician (e.g., Crystal_Thomas)

### 1.2.4 Ex Header
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ start ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/begin_header	
/identifier_product_doi=10.5067/SeaBASS/EXPORTS/DATA001	
/received=20190807	
/affiliations=Bowdoin_College	
/investigators=Collin_Roesler	
/contact=croesler@bowdoin.edu	sdrapeau@bowdoin.edu
/experiment=EXPORTS	
/cruise=EXPORTSNP	
/data_file_name=EXPORTS_EXPORTSNP_HPLC-inline_survey_20180814.csv	
/original_file_name=EXPORTS_Roesler_08-06_report.xlsx	
/data_type=pigment	
/start_date=20180811	
/end_date=20180908	
/start_time=19:19:00[GMT]	
/end_time=19:44:00[GMT]	
/water_depth=na	
/measurement_depth=5.5	
/west_longitude=-145.960[DEG]	
/east_longitude=-131.543[DEG]	
/north_latitude=50.643[DEG]	
/south_latitude=49.4265[DEG]	
/documents=EXPORTS_Method_Inline_HPLC_Roesler_SeaBASS.docx	EXPORTS_2018_DATA_MASTER.xlsx
/calibration_files=EXPORTS_Method_Inline_HPLC_Roesler_SeaBASS.docx	
/data_status=final	
/missing=-9999	
/below_detection_limit=-8888	
/HPLC_lab=NASA_GSFC	
/HPLC_lab_technician=Crystal_Thomas	
/delimiter=comma	
/fields=hplc_gsfc_id	sample
/units=none	none
/end_header	

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

## 1.3 Data fields
1. Individual and summed pigments (based on SeaBASS fields/SeaHARRE reports)

2. Separate columns for any size-fractionated measurements (e.g., Tot_Chl_a_20umprefilt goes in a separate column from Tot_Chl_a)

3. Include "hplc_gsfc_id" as a column if your data were analyzed at the NASA GSFC lab to preserve the lab's sample ID

Report any replicate filters separately
Use the /below_detection_limit to mask any relevant values. It is distinct from the /missing value (e.g., /missing=-9999 vs. /below_detection_limit=-8888)
SeaBASS files may only contain data from a single cruise or deployment. If necessary, sort and separate your data by cruise then create multiple SeaBASS files
An online tool (HPLC2sb) is optionally available to help convert NASA GSFC HPLC spreadsheets into SeaBASS file format. However, please read its limitations and caveats carefully. https://seabass.gsfc.nasa.gov/hplc2sb/

# 2.0 Start Code:
## Load Libraries
```{r libraries}
librarian::shelf(
  librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
  forcats, lubridate, glue, fs, magrittr, here,
  
  # additional
  hms, janitor
)

library("conflicted")

conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

# load consolidated HPLC by cruise
(dir_ls(here("data", "processed", "hplc"),
       regexp = "RData") %>%
  sort(decreasing = TRUE))[1] |>
  load() 

# location to save files
loc <- 
  here("data", "processed", "hplc", glue("ind_file_{Sys.Date()}"))
```

## All SeaBass Standard Fields 
https://seabass.gsfc.nasa.gov/wiki/stdfields
```{r}
sb_field <- 
  read_tsv(here("data", "metadata", "seabass_standard_fields_all.txt"),
           show_col_types = FALSE,
           name_repair    = make_clean_names) %>%
  mutate(
    .after   = 1,
    low_name = str_to_lower(field_name),
    low_name = str_replace_all(low_name, "-", "_")
         )

# extract all field names from data and map to SeaBass standard names
hplc_data <- hplc_data2 <- 
  hplc_data %>% 
  mutate(
    # edit old names from previous report to match current SeaBass
    sb_name = map(
      .x = names,
      (\(x) 
            x  %>%
            mutate(
              low_name = str_replace(param_name, "(?i)t_caro", "tcar"),
              low_name = str_replace(low_name, "t_pg|t_pig", "tpg"),
              low_name = case_when(
                str_detect(low_name, "chl_c12") ~ "chl_c1c2",
                str_detect(low_name, "ppc_tpig") ~ "ppc_tpg",
                str_detect(low_name, "t_chl_a_tpg") ~ "tchla_tpg",
                str_detect(low_name, "tchl_a") ~ str_replace(low_name, 
                                                             "tchl_a", "tchla"),
                str_detect(low_name, "^t_chl") ~ str_replace(low_name, 
                                                             "t_chl", "tchl"),
                str_detect(low_name, "t_acc") ~ str_replace(low_name, 
                                                            "t_acc", "tacc"),
                .default = low_name)
              ) %>%
              left_join(sb_field, by = c("low_name"))))
    ) 
```
## Header Information
```{r default}
contacts <- 
  tribble(
    ~invest,               ~email,
    "Frank_Muller-Karger", "carib@usf.edu", 
    "Daniel_Otis",         "dotis@usf.edu", 
    "Digna_Rueda_Roa",     "druedaro@usf.edu"
)


hd_def <- list()
hd_def$prep     <- "Sebastian Di Geronimo" # <----------------------- edit here
hd_def$invest   <- str_c(contacts$invest, collapse = ",")
hd_def$contact  <- str_c(contacts$email, collapse = ",")
hd_def$affil    <- "University_of_South_Florida,USA"
hd_def$expri    <- "SFMBON"
hd_def$hplc_lab <- "NASA_GSFC"	
hd_def$lab_tech <- "Crystal_Thomas"	
hd_def$type     <- "pigment"
hd_def$status   <- "final" # final - not intending to revisit
hd_def$wtr_dpth <- hd_def$stn <- "na"

hd_def$suffix <- "R1"
hd_def$fields_extr <- 
  tibble(
    field_extr = c("hplc_gsfc_id", "sample", "date","time", "station",
                   "lon","lat","depth", "water_depth","volfilt")) %>%
  left_join(sb_field, by = c("field_extr" = "field_name"))

# TODO: fill in
# needs: checklist, cruises report?
hd_def$docs     <- "__TODO__"


hd_def$comments <-
  glue(.sep = "\n",
       "!",
       "! COMMENTS",
       "! Prepared by: {hd_def$prep}",
       "! Creation Date: {format(Sys.Date(), \"%B %d, %Y\")}",
       "!",
       "! Samples were processed at NASA GSFC by Crystal Thomas",
       "!",
       "! Values of -9999 represent missing information;",
       "!",
       "! Values of -8888 represent measurements below detection limits (LOD),",
       "! this represents pigments not detected or pigments present with", 
       "! concentrations less than 0.0005 mg/m^3."
       ) 

hplc_data <- 
  hplc_data %>%
  mutate(
    precise_comm = map_chr(
      precision,
      (\(xl) 
       if (any(str_detect(as.matrix(xl), "replicate"), na.rm = TRUE)) {
         glue("!")
         } else if (nrow(xl) > 1) {
         glue(
           "!\n! Coefficient of variation (replicate filter precision)",
           "\n! TChl a = {str_c(xl$t_chl_a, collapse = \",\")};",
           "\n! Ppig   = {str_c(xl$ppig, collapse = \",\")}",
           "\n!") 
         } else {
        glue(
           "!\n! Coefficient of variation (replicate filter precision)",
           "\n! TChl a = {xl$t_chl_a};",
           "\n! Ppig   = {xl$ppig}",
           "\n!")
        }
       )),
    cal_file = map_chr(data, \(x) x$cal_files[[1]])
    )
```

```{r headr-skeleton}
header_skeleton <-
  glue::glue(
    "/begin_header",
    # required  
    "/investigators={hd_def$invest}",
    "/affiliations={hd_def$affil}",
    "/contact={hd_def$contact}",
    "/experiment={hd_def$expri}",
    "/cruise={cruise_id}",
    "/station={hd_def$stn}",
    "/data_file_name={file_name}", # current name of data file, self-reference
    # "/documents={hd_def$docs}",    # additional info about experiment and cruise
    "/documents={doc}",    # additional info about experiment and cruise
    "/data_type={hd_def$type}",
    "/calibration_files={cal_file}",
    "/start_date={start_date}",
    "/end_date={end_date}",
    "/start_time={start_time}[GMT]",
    "/end_time={end_time}[GMT]",
    "/north_latitude={north_latitude}[DEG]",
    "/south_latitude={south_latitude}[DEG]",
    "/east_longitude={east_longitude}[DEG]",
    "/west_longitude={west_longitude}[DEG]",
    "/water_depth={hd_def$wtr_dpth}",
    "/missing=-9999",
    "/delimiter=comma",
    
    
    # HPLC required
    "/HPLC_lab={hd_def$hplc_lab}",
    "/HPLC_lab_technician={hd_def$lab_tech}",
    "/below_detection_limit=-8888",
    
    # conditional 
    
    # optional recommended
    "/data_status={hd_def$status}",
    
    # optional
    "/original_file_name={fbase}", 
    
    "{hd_def$comments}",
    "{pc_com}",
    "/fields={fields}", # fields of data 
    "/units={pun}",  # units for each column 
    "/end_header", 
    .sep = "\n") |>
  
  expression()
```

## Extract Header Information
```{r header-info}
hplc_w_head <-
  hplc_data %>%
  mutate(
    header_info = map(
      .x = data,
      function(x) {
        x %>%
        mutate(datetime = date + time) %>% # create datetime 
          
        # get min and max of head per cruise header info
        summarise(across(.cols = c(datetime, lon, lat), 
                         # new way using lambda notation
                         .fns = list(min = (\(x) min(x, na.rm = TRUE)),
                                     max = (\(x) max(x, na.rm = TRUE))),
                         .names = "{.col}_{.fn}")) %>%
        
        mutate(
          # split datetime to date and time separately
          start_date = format(ymd(as_date(datetime_min)),"%Y%m%d"),
          end_date   = format(ymd(as_date(datetime_max)),"%Y%m%d"),
          start_time = as_hms(datetime_min),
          end_time   = as_hms(datetime_max),
          
          # rename bound box variable names
          north_latitude = round(lat_max, 3),
          south_latitude = round(lat_min, 3),
          east_longitude = round(lon_max, 3),
          west_longitude = round(lon_min, 3),
          .keep = "unused")
        }
      ),
    # edit doc when have more documents 
    doc = glue("{rtf_file}", .na = "na")
    )

hplc_w_head
```

```{r header-info}
# if want to see header output, set to true
verb <- FALSE

# cruise data header_info cruise_id 
final <-
  hplc_w_head %>%
  # slice(1) %>%
  mutate(
     heads = pmap(
      .progress = TRUE,
      .l = list(data, header_info, cruise_id, sb_name, precise_comm, cal_file,
                doc),
      verb = verb,
      function(y, x, cruise_id, sb_name, pc_com, cal_file, doc, verb){
        # remove spaces in original base name 
        fbase <- 
          str_replace_all(unique(y$base), " ", "_") %>%
          str_c(collapse = ",")
        
        
        # create file name based on cruise ID
        file_name <-
          glue("{hd_def$expri}", "{cruise_id}", 
               "{hd_def$type}", "{x$start_date}_{hd_def$suffix}.sb", 
               .sep = "_")
        
        # fields and units standards: https://seabass.gsfc.nasa.gov/wiki/stdfields
        # SeaBass standard fields
        field_nm <- c(hd_def$fields_extr$field_extr, sb_name$field_name)
        fields   <- str_c(field_nm, collapse = ",")
        
        # units each field
        pun <- str_c(c(hd_def$fields_extr$units, sb_name$units), collapse = ",")
        
        # evaluate header
        header <- with(x, eval(header_skeleton)) 
        
        if (verb) {
          cat("\n-----\n", cruise_id, "\n\n")
          print(header)
          cat("\n-----\n\n")
        }

        # return values
        return(
          tibble(
            header    = header,
            file_name = file_name,
            fld_use   = list(field_nm = field_nm)))
        }
     ),
     cols = pmap(
      list(data, heads, sb_name),
      function(.x, .y, .z) {
        select(.x, any_of(c(hd_def$fields_extr$field_extr,
                            .z$param_name))) %>%
          purrr::set_names(.y$fld_use[[1]])
        }
      )
     )
```


## Saving Files in SeaBass Format
```{r save-files}
dir_create(loc) # create dir for each day running

# save files
pwalk(list(final$cols, final$heads),
      function(x, y) {
        # write header
        cat(y$header, "\n", sep = "",
            file = here(loc, y$file_name)
            )
        
        x %>%
          # convert all to characters
          mutate(
            date = format(date,"%Y%m%d"),
            across(everything(), \(x) as.character(x)), # convert to char
            across(everything(), \(x) case_when(
                    is.na(x) ~ "-9999", # chg NA to -9999
                    .default = x))
           ) %>%

          # filter out rows where no data exists
          filter(!if_all(date:last_col(), \(x) x == "-9999")) %>%

          # write data after header
          write_delim(.,
                      delim     = ",",
                      file      = here(loc, y$file_name),
                      append    = TRUE,
                      col_names = FALSE
                      )
        }
      )
```

# Read SeaBass HPLC Files and Check for Issues
All columns are read as characters to check for any issues with the data
```{r read-seabass}
# function to read seasbass hplc files and sort through head, and data
read_sb_files <- function(x) {
  skips <-
    read_lines(x) %>% str_detect("end_header") %>%
    which()
  
  head <-
    read_lines(x) %>% str_detect("fields") %>%
    which()
  
  col_names <-
    read_csv(
      x,
      skip = head - 1,
      n_max = 1,
      col_names = FALSE,
      show_col_types = FALSE
    ) %>%
    mutate(X1 = str_remove(X1, "/fields=")) %>%
    as.character()
  
  read_csv(
    x,
    skip = skips,
    col_names = col_names,
    show_col_types = FALSE,
    col_types = cols(.default = "c"),
    # na = c("", "NA", "-9999")
    na = c("", "NA")
  )
}

# get most recent directory created for seabass files
ind_f_dir <- 
  here("data", "processed", "hplc") %>%
  dir_ls(type   = "directory", 
         regexp = "ind_file") %>%
  sort(decreasing = TRUE)

# read seabass hplc files
sb_data <-
  ind_f_dir[1] %>%
  dir_ls() %>%
  tibble(file = .) %>%
  mutate(data = map(file,
                    read_sb_files)) %>%
  unnest(data)

message(glue("The dimensions are: {dim_desc(sb_data)}"))
slice_head(sb_data, n = 10)

sb_data %>%
  filter(if_any(sample:volfilt, \(x) is.na(x) | str_detect(x, "-9999")))
```

# FCHECK
This is used by NASA to pre-check the .sb files before official submission 
process. This requires the use of "Perl" <https://www.perl.org/get.html>.
A quick download and it should work.
```{r}
# Toggle if want to see fcheck out put or look just for errors
verb2 <- FALSE
verb2 <- T

tryCatch({
  system2("perl", "-v", stdout = TRUE)
  invisible()
  }, error = function(e) {
    # message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
    stop(glue("Perl does not seem to be downloaded to your computer.\n", 
              "To continue you will need to download perl at: ",
              "https://www.perl.org/get.html\n",
              "----\n",
              "Perl is used to run the fcheck.pl script to check the .sb files"))
    
      })

test <- 
  dir_ls(ind_f_dir[1]) %>%
  tibble(files = .) %>%
  # slice(1) %>%
  slice_tail(n = 1) %>%
  mutate(
    fcheck2 = map(files, 
                 \(x) paste( here("fcheck", "fcheck4.pl"), x) %>%
                   system2("perl", ., stdout = TRUE))
  ) %T>%
  {if (verb2) {
    pull(., fcheck2) %>%
    print()
  } else {.}}

# print file and summary of errors/warnings
for (i in seq(nrow(test))) {
  print(glue(
    basename(test$files[[i]]),
    test$fcheck2[[i]][which(str_detect(test$fcheck2[[i]], "\\d+.*error"))],
    .sep = ": "))
}


```
# Get Date Range of Each Cruise
```{r}
dt_rng <- 
  sb_data %>%
  mutate(
    .keep = "none",
    cruise   = str_extract(sample, "\\w{2,3}\\d{4,5}"),
    date     = as_date(date),
  ) %>%
  summarise(
    .by = cruise,
    num = n(),
    # date_min = as_date(date_min, format = ""),
    date_max = max(date)
  ) %>%
  arrange(date_max) %>%
  mutate(
    .before = date_max,
    date_min = str_extract(cruise, "\\d+"),
    date_min = as_date(date_min, format = "%y%j"),
    date_min = format(date_min, "%b %d"),
    date_max = format(date_max, "%b %d %Y")
    ) %>%
  unite(col = "date_range", date_min, date_max, sep = " - ") 

df_rng

if (FALSE) {
  write_csv(
    dt_rng,
    file = here("date_range.csv"),
    quote = "none"
    )
}
```



/documents=
checklist_hplc_EXPORTS-EXPORTSNA_SR1812.rtf,
EXPORTS_Roesler_08-06_report_rev2.xlsx,
EXPORTS_pump_log_data_work_up_Sally_Ride_August_2018_QMA_SUPOR_organics.xls,
EXPORTSNP_SR1812_Roesler_sample_log_V2.xlsx,
R2R_ELOG_SR1812_FINAL_EVENTLOG_20180913_022931.xlsx


# Copy files to box
If you want to copy files from local to cloud directory, you may run this
chunk. 

By default, it will skip. 
```{r coy-to-box}
shelf(cli)
  cloud_loc <- here(cloud_dir, "hplc", "seabass_submit")
  ovrwrt    <- FALSE
  wrt_cld   <- FALSE
  
  # cloud_loc <- here("data", "test")
if (!wrt_cld) {
  cli_alert_info("Skipping all files!")
  cli_alert_warning(c("If you want to save files to {.file {cloud_dir}}, ",
                     "set {.var wrt_cld} = {.var TRUE}",
                     "(currently set to {.var {wrt_cld}})"))
  } else {
  if (!ovrwrt) {
    cli_text("\n\n")
    cli_alert_warning("You might be overwriting previsouly saved data.")
    cli_alert_warning(c("If this was a mistake, set {col_red(\"ovrwrt\")} ",
                        "= {.var FALSE}"))
    cli_text("Hit {col_red(\"\\\"enter\\\"\")} to continue or 
             {col_red(\"\\\"esc\\\"\")} to escape and correct.")
    invisible(readline(""))
  }

  file_to_box <-
    reduce(
      .f = left_join, 
      by = join_by(cruise_id),
      list(
        # extract location for each calibration file
        select(hplc_data, cruise_id, cal_file) %>%
        mutate(
          cal_file = map_chr(.x = cal_file,
                             ~ here("data", "raw", "hplc") %>%
                               dir_ls(regexp = .x))
          ),
        
        # extract cruise ID and path to individual seabass submission files
        (here("data", "processed", "hplc") %>%
           dir_ls(regexp = "ind_file") %>%
           sort(decreasing = TRUE))[1] %>%
          dir_ls() %>%
          tibble(sb_file = .) %>%
          mutate(
            cruise_id = str_extract(sb_file, "((WS|WB|SV)[0-9].*)(?=_p)")
            ),
        
        # extract location for each cruise checklist      
        here("data", "raw", "hplc", "checklist") %>%
          dir_ls() %>%
          tibble(chk_file = .) %>%
        mutate(
          cruise_id = str_extract(chk_file, "((WS|WB|SV)[0-9].*)(?=\\.)")
          )
        )
      ) %>%
    # add cloud directory for each cruise
    mutate(cloud_dir = here(cloud_loc, cruise_id)) %>%
    nest(.by = c(cruise_id, cloud_dir)) %$%
    
    pwalk(list(cruise_id, data, cloud_dir),
          function(x, y, dir_loc) {
            cli_h1(x)
            cli_alert_info("Copy Location: {.file {dir_loc}}")
            y <- 
              pivot_longer(
                data      = y,
                cols      = everything(),  # columns to pivot long,
                names_to  = "file_type", # desired name for category column
                values_to = "file_path", # desired name for value column
                )  %>%
              filter(!is.na(file_path)) 
            
          walk2(
            .x = y$file_path,
            .y = dir_loc,
            function(from, to) {
           dir_create(to)
           tryCatch({
             file_copy(from, to, overwrite = ovrwrt)
             cli_alert_success("Copying: {.file {basename(from)}}")
             }, error = function(e) {
               cli_alert_danger(c("Skipping: {.file {basename(from)}} ",
                                  "{col_red(\"Already exists!\")}"))
             })
              })}
        )
  
  if (!ovrwrt) {
    cli_text("\n\n")
    cli_alert_warning(c("If you want to {col_red(\"overwrite\")} existing ",
                       "files, set {.var ovrwrt} = {.var TRUE}",
                       "(currently set to {.var {ovrwrt}})"))
  }
} 
```

# Save Consolidated Info and Select Variables
```{r}
hplc_filt <-
  hplc_data2 %>%
  mutate(
    data = map2(
      .x = data,
      .y = sb_name,
      function(x, y) {
        
        x <- select(x, !contains(c("file", "base", "cal_files")))
        
        new_names <- tibble(og_names = names(x))  %>%
        left_join(y, by = join_by(og_names == param_name)) %>%
        mutate(
          field_name = if_else(is.na(field_name), og_names, field_name)
        )
        
        new_names$field_name
      
        names(x) <- new_names$field_name
        return(x)
        }
      
      )
  ) %>%
  select(1:3) %>%
  unnest(data) 


if (FALSE) {
    write_csv(hplc_filt, here("data", "processed", "hplc", 
                             glue("hplc_consolidate_select_vars_",
                                  "{Sys.Date()}",
                                  ".csv")))
  if (FALSE) {
    write_csv(hplc_filt, here(cloud_dir, "hplc",
                              glue("hplc_consolidate_select_vars_",
                                   "{Sys.Date()}",
                                   ".csv")))
    }
  }
```

x  hdr = {'/begin_header';
  strcat('/received=',date_received);
  '/investigators=Frank_Muller-Karger,Enrique_Montes';
  '/affiliations=University_of_South_Florida,USA';
'/contact=emontesh@usf.edu';
'/experiment=MBON';
strcat('/cruise=',cruise);
strcat('/data_file_name=',cruise,'_',data_type);
strcat('/documents=',cruise_report);
'/calibration_files=HPLC_method_summary';
'/delimiter=space';
'/data_type=pigment';
'/data_status=final';
strcat('/start_date=',start_date);
strcat('/end_date=',end_date);
strcat('/start_time=',start_time,'[GMT]');
strcat('/end_time=',end_time,'[GMT]');
strcat('/north_latitude=',lat_n,'[DEG]');
strcat('/south_latitude=',lat_s,'[DEG]');
strcat('/west_longitude=',lon_w,'[DEG]');
strcat('/east_longitude=',lon_e,'[DEG]');
'/below_detection_limit=-8888';
'/missing=-8888';
'/water_depth=-999';
'/cloud_percent=-999';
'/wave_height=-999';
'/wind_speed=-999';
'/secchi_depth=-999';
'!';
'!RUN BY: Enrique Montes';
'!Samples were processed at NASA GSFC by Crystal Thomas';
'!Measurements below detection limit are assigned the value -9999; this represents pigments not detected';
'!as well as pigments present but with concentrations less than 0.0005?g/L.';
'!Coefficient of variation (replicate filter precision) for TChl a =	1.64%; Ppig =	3.74%';
'!!Sky: variable conditions';
'!Water: variable conditions.';
'/fields=date,station,lon,lat,depth,Tot_Chl_a,Tot_Chl_b,Tot_Chl_c,alpha-beta-Car,But-fuco,Hex-fuco,Allo,Diadino,Diato,Fuco,Perid,Zea,MV_Chl_a,DV_Chl_a,Chlide_a,MV_Chl_b,DV_Chl_b,Chl_c1c2,Chl_c3,Lut,Neo,Viola,Phytin_a,Phide_a,Pras,Gyro,TChl,PPC,PSC,PSP,Tcar,Tacc,Tpg,DP,Tacc_TChla,PSC_Tcar,PPC_Tcar,TChl_Tcar,PPC_Tpg,PSP_Tpg,TChla_Tpg';
'/units=yyyymmdd,none,degrees,degrees,m,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,mg/m^3,none,none,none,none,none,none,none';
'/end_header'};




