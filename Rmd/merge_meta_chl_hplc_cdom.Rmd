---
title: "Read All Meta for eDNA"
author: "Sebastian DiGeronimo"
date: "2022-09-13"
output: html_document
---

# TODO: format like merge_data.Rmd
# ---- Load Libraries ----
```{r setup}
librarian::shelf(
  librarian, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
  forcats, lubridate, glue, fs, magrittr, here,
  # broom # optional
  
  # additional
  readxl
)

library("conflicted")

conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

# `NA` values to skip when reading `.xlsx.
na_skip <- c("NA", "Skipped", "skipped", "na", "n/a", "n./a", "n.a", "Flow", 
             "*", stringi::stri_dup("-", 1:20))

# where to pull data from
# "cloud" - cloud path ~/<box-location>/mbon_imars_cruises/
# "local" - local path in this project, ~/data/metadata/
locat <- "cloud" 
```

# ---- Search files ----
## Cloud Directory
To make searching faster, the first time you run this it will search the cloud
directory location for the folder names with `~/<year>/<cruise>/<metadata>`.
This will take a few minutes, but it will save in the root of this project. 

When re-running, it will only look at previous `box_search`, so you may need to
re-run if new folders are added.

After the first time, you will get a variable called `cloud_dir`. You should 
copy this into a `.Rprofile` so when you restart `R`, the path will be set.

i.e. copy "cloud_dir <- `<path-to-box>/mbon_imars_cruises`" 
*make sure to change `<path-to-box>` to the location on your computer.*
```{r find-files-box}
if (!exists("cloud_dir")) cloud_dir <- rstudioapi::selectDirectory()
box_search2 <- 
  search_cloud_folders(
    .cloud_dir  = here(cloud_dir, "years"),
    .sub_folder = list("metadata"),
    return_type = "vector") %>%
  dir_ls(
    regexp = "\\.(xlsx)$"
  ) %>%
  str_subset("~|ignore", negate = TRUE) %>%
  tibble(file_path = .,
         base  = basename(.)) %>%
  filter(str_detect(file_path, "(?i)fknms_")) %>%
  mutate(
    cruise_id = str_extract(file_path, "\\w{1,2}\\d{4,5}"),
    year = str_extract(file_path, "[0-9]{4}"),
    year = as.numeric(year)
  ) %T>% print()
```


```{r find-files-box}
# folder path in cloud directory
if (!file_exists(here("box_search.csv"))) {
  # if need box_search.csv 
  if (!exists("cloud_dir")) cloud_dir <- rstudioapi::selectDirectory()
  # super inefficient
  # box_search <- 
  # dir_ls(
  #   here(cloud_dir, "years"),
  #   regexp  = "metadata",
  #   type    = "directory",
  #   recurse = TRUE) %>%
  #   tibble(folder = .)
  
  write_csv(box_search, here("box_search.csv"), append = TRUE)
} else {
  box_search <- read_csv(here("box_search.csv"), show_col_types = FALSE)
}

files_all <-
  dir_ls(box_search$folder,
         regexp  = "\\.(xlsx)$",
         recurse = TRUE) %>%
  str_subset("~|ignore", negate = TRUE) %>%
  tibble(file_path = .,
         base  = basename(.)) %>%
  mutate(
    # cruise_id = str_replace(file_path, ".*cruises/[0-9]+/(.*)/m.*", "\\1"),
    cruise_id = str_extract(file_path, "\\w{1,2}\\d{4,5}"),
    
    year = str_extract(file_path, "[0-9]{4}") %>%
           as.numeric()
  ) %T>% print()

files_filt <- 
  files_all %>%
  filter(str_detect(file_path, "(?i)fknms_")) %T>% 
  print()
```

## Local Directory
This is the location within this project that should be created when loading the
first time.
```{r find-files-local}
files_local <-
  dir_ls(here("data", "metadata"),
         regexp  = "^[^~]*\\.(xlsx)$",
         recurse = TRUE) %>%
  tibble(file_path   = ., 
         base    = basename(.)) %>%

  # ignore some files
  filter(str_detect(basename(file_path), "fknms_") &
         !str_detect(file_path, "ignore")) %T>% 
  print()
```
## Compare Local and Cloud
Check the differences between the two to see if any discrepencies exists. 
You should only need to use the `cloud_dir` location
```{r compare}
files_local$base %in% files_filt$base
files_all$base %in% files_local$base

anti_join(files_all, files_local,   by = "base")
anti_join(files_filt, files_local,  by = "base")
inner_join(files_filt, files_local, by = "base")

full_join(files_filt, files_local,  by = "base") %>%
  arrange(year, base) %>%
  filter(is.na(file_path.y)) 
```

# ---- Extract sheet name and row to read ----
Extracts sheet info by looking for prefix `field` or `Sheet1`
- if none are found, it will ask which of the sheets it could be

Extracts row info where data starts by looking for prefix `Sample`
- if none are found, it will ask which row it could be

Extracts how many columns to read in. This looks for either `notes` or 
`collector` as the last column.
```{r sheet-info}
files <- switch(locat,
  "cloud" = files_filt,
  "local" = files_local
)

inventory <- dir_ls(cloud_dir, regexp = "imars_inventory")

prev_cruise_id <- 
  inventory %>%
  openxlsx::read.xlsx(
    sheet = "cruise_summary"
  ) %>%
  pull(cruise_id)

files <- 
  files %>%
  # slice_tail(n = 5)
  filter(!cruise_id %in% prev_cruise_id)


files <- get_sheet_info(files)

files
```


```{r sheet-info}
# recal <- tibble()
# 
# for (i in seq(nrow(files))) {
#   # select sheet name with `field_logsheet`
#   cli::cli_alert_info("Getting sheet info for file: {.file {files$base[i]}}")
#   temp_sht <- excel_sheets(files$file_path[i]) 
#   sht_num  <- which(str_detect(temp_sht, "field_"))
# 
#   if (is_empty(sht_num)) {
#     sht_num <- which(str_detect(temp_sht, "Sheet1")) 
#   } 
#   
#   if (is_empty(sht_num)) {
#     sht_num <- menu(temp_sht, 
#                      title = glue(
#                        "\n-----\n\n",
#                        "Which sheet contains metadata?", 
#                        "\n(0 for none)"))
#   }
#   
#   # skip if no sheet name
#   if (sht_num == 0) {
#     recal <- 
#       bind_rows(
#         recal,
#         tibble(file_path = files$file_path[i]))
#     next
#   }
#   
#   # read sheet 
#   temp <- read_excel(
#     files$file_path[i],
#     sheet        = sht_num, 
#     n_max        = 5,
#     col_names    = FALSE,
#     .name_repair = "unique_quiet") 
# 
#   # row number that contains headers
#   row <- which(apply(temp, 1, function(x) any(grepl("(?i)sample", x))))
#   
#   if (is_empty(row) || is.na(row)) {
#     View(temp)
#     row <- readline("Which line is the header? ") %>%
#            as.numeric()
#   }
#   
#   # skip if no row info
#   if (is_empty(row) || is.na(row)) {
#     recal <-
#       bind_rows(recal, 
#               tibble(file_path    = files$file_path[i], 
#                      sht_num = sht_num))
#     next
#     }
#   
#   # get last column to read
#   # either `notes` or `collector`
#   last_c <- which(grepl("(?i)notes|(?i)collector", temp[row,])) %>%
#             max()
#   
#   recal <- 
#     bind_rows(
#       recal,
#       tibble(
#         file_path = files$file_path[i],
#         sht_num = sht_num,
#         row     = row,
#         last_c  = last_c
#       )
#     )
# }
# 
# # joins recal and files
# files <- recal %>%
#   drop_na(everything()) %>%
#   left_join(files, by = "file_path")
# 
# rm(recal)
```
# ---- Read all data as strings and merge ----
No filtering of data is done. It will remove all `NA`s that was specified above.
```{r all-data-as-str}
og_data_chr <-
  suppressMessages(
    files %>%
      mutate(
        data = pmap(
          list(file_path, sht_num, row, last_c),
          function(.x, .y, .z, .l) {
            temp <- read_xlsx(
              path         = .x,
              sheet        = .y,
              skip         = .z - 1,
              range        = cell_cols(c(NA, .l)),
              .name_repair = janitor::make_clean_names,
              na           = na_skip,
              col_types    = "text"
            ) %>%
              # remove column
              select(-any_of("time_filtered_24_00"))
          }
        )
      ) %>%
      unnest(data) %>%
      # combine different time column names into one
      unite(
        "sample_collection_time_gmt",
        contains(c("sample_collection_time_gmt", 
                   "time_gmt", 
                   "time_sampled_24_00")),
        sep    = "",
        na.rm  = TRUE,
        remove = TRUE
      ) %>%
      # convert to time by converting decimal (fraction of day in second) to HMS
      # convert decmial to seconds then convert to time
      # 24*60*60 = 86400 seconds in a day
      # i.e. 0.4826388888888889 = 41700 seconds
      # then hms::as_hms(41700) = 11:35:00
      type_convert() %>%
      mutate(
        sample_collection_time_gmt = 
          hms::as_hms(sample_collection_time_gmt * 24 * 60 * 60)
      )
  )
```
# ---- Load data and correct formats if needed ----
Loads all data and fixes column formatting to merge afterwards.
This ignores potential NAs using any of 
- "NA", "Skipped", "skipped", "na", "n/a", "n./a", "n.a", "Flow", "*", or any 
  number of "-"s (i.e "-", "--", etc)
```{r load-data}
files <-
  files %>%
  # filter(cruise_id == "H23138") %>%
  
  mutate(
    data = pmap(
      list(file_path, sht_num, row, last_c), 
      read_logsheets
      )
    ) 

files$data
```
# ---- Check Mismatched ----
Check which sheets and columns have different types from the rest so you can fix
using the above code. 

Currently, as of `Feb 1st, 2023`, there are no mismatches.
```{r mismatch}
mismatch <- t(janitor::compare_df_cols(files$data, return = "mismatch"))        
rownames(mismatch) <- c(NA, tools::file_path_sans_ext(files$base))
                         
mismatch

# # examine issue columns by entering the name of the file and the column that 
# # might be problematic
# # bad <- 
# (files %>%
#   filter(str_detect(file_path, "<enter-bad-base-file-name>")) %>%
#   pull(data))[[1]]  %$% 
#   unique(<variable-to-check>) 
# 
# # open file in excel if on windows?
# files %>%
#   filter(str_detect(file_path, "<enter-bad-base-file-name>")) %$% 
#   shell.exec(file_path)

# checks if any dates are betfore 2015. This would show bad dates and will need
# to be fixed
# An error means no bad dates
files %>%
  unnest(data) %>%
  filter(!is.na(vol_ml) & 
         date_mm_dd_yy <= as_date("2015-01-01")) 
```

# ---- Summarise Results ----
```{r unnest}
data_unnest <-
  files %>%
  unnest(data) %>% 
  mutate(station = str_remove(station, "~"),
         station = str_replace_all(station, " ", ""),
         station = str_to_upper(station),
         station = case_when(
           str_detect(station, "9.69999") ~ "9.7",
           str_detect(station, "9.80000") ~ "9.8",
           TRUE ~ station)) %>%
  select(-c(file_path:last_c)) %>%
  
  filter(
     # removes rows that contain no data encoded as `1899-12-31 00:00:00` or `NA`
     # and vol_ml is `NA`
    !( (date_mm_dd_yy < as_date("2015-01-01") | is.na(date_mm_dd_yy))
      & is.na(vol_ml))
    
    # filter where sample_type (chl|hplc|cdom) has no vol and no date/time
    & !(is.na(date_time) & is.na(vol_ml))) %>%
  
  group_by(cruise_id, station, depth_m) %>%
  filter(!(
    !is.na(notes) 
    &
      str_detect(notes, "skipped b/c low on amber vials")) 
    & 
      if_any(vol_ml, ~ !any(
        str_detect(sample_type, "(?i)chl|hplc") 
        & (is.na(.x) | .x == 0))) 
    & 
      !(!is.na(notes) 
        & str_detect(
          notes, 
          "Not taken|not enough water|no more cdom bottles|sample was not taken|sample was lost")))

data_unnest %$% unique(station)
```


```{r station-summary}
data_summary <- 
  data_unnest %>%
  mutate(vol_ml = if_else(is.na(vol_ml) | vol_ml == 0, -9999, vol_ml)
  ) %>%
    select(-identifier, -sample_number, -notes, -c(1:base), -collector) %>%
    pivot_wider(
    data         = .,
    names_from   = c(sample_type), # category column(s) to pivot wide
    values_from  = c(vol_ml), # value column(s) that hold data for each category column
    names_sep    = "_",
    names_repair = janitor::make_clean_names,
    values_fn    = list(vol_ml = length)
    ) %>%
      ungroup()

data_summary
```


```{r cruise-summary}
cruise_summary <- 
  data_summary %>% 
  summarise(
    .by  = cruise_id,
    date = min(date_mm_dd_yy, na.rm = TRUE),
    across(chl_a:cdom, sum, na.rm = TRUE)
    ) %>%
  mutate(
    date   = as_date(date),
    year   = year(date),
    month  = month(date),
    .after = date) %>%
  arrange(date) 

cruise_summary
```


```{r sample-progress}
sample_progress <- 
  data_unnest %>%
  ungroup() %>%
  transmute(
    cruise_id,
    station,
    identifier, 
    sample_type,
    present_missing   = NA,
    collection_date   = date_time, 
    collected_by      = collector,
    processed_date    = NA_Date_,
    processed_by      = NA,
    post_process_date = NA_Date_,
    post_process_by   = NA,
    submit_date       = NA_Date_,
    submit_database   = NA,
    submit_by         = NA
         ) %>%
    group_by(sample_type) %>%
    nest()


# cruise_summary %>%
#   mutate(
#     chl_analyzed = glue("of {chl_a}"),
#     hplc_analyzed = glue("of {hplc}"),
#     cdom_analyzed = glue("of {cdom}"),
#   )
View(cruise_summary)
```

## WIP: Append Data to Inventory
```{r}
unshelf(readxl)
shelf(openxlsx)

test_list <- list(
  "cruise_summary"   = cruise_summary,
  "chl_progress"     = sample_progress$data[[1]],
  "hplc_progress"    = sample_progress$data[[2]],
  "cdom_progress"    = sample_progress$data[[3]],
  "station_summary"  = data_summary,
  "sample_meta_data" = data_unnest
)

wb <- loadWorkbook(inventory)

getSheetNames(inventory)

for (i in getSheetNames(inventory)) {
  print(i)
  
  # ---- reorder data for sheet `sample_meta_data`
  if (i == "sample_meta_data") {
    test_list[[i]] <- select(test_list[[i]], names(readWorkbook(wb, i)))
  }

  writeData(
    wb       = wb,
    sheet    = i,
    x        = test_list[[i]],
    colNames = FALSE,
    startRow = nrow(
      readWorkbook(
        xlsxFile =  wb,
        sheet    = i
      )
    ) + 2
  )
}

openXL(wb)
```


```{r}
# wb <-
#   inventory %>%
#   openxlsx::getSheetNames() %>%
#   tibble(sheets = .) %>%
#   mutate(
#   data = map(
#     .x = sheets,
#     inventory,
#     .f = function(x, inventory) {
#       dat <- 
#         openxlsx::read.xlsx(xlsxFile = inventory, sheet = x) %>%
#         mutate(
#           try(across(contains("date"),
#                  ~ janitor::excel_numeric_to_date(.x,
#                                                   include_time = TRUE,
#                                                   round_seconds = TRUE)))
#         ) %T>% 
#         print()
#         
#       if (any(str_detect(names(dat), "sample_number"))) {
#         dat <-
#           mutate(dat,
#                  sample_number = as.character(sample_number))
#       }
#         # janitor::excel_numeric_to_date(date) %T>% print()
#         return(dat)
#     }
#   )
# ) 
# 
# date 
# collection_date = 42236.482639
# date_mm_dd_yy 
# date_time
# 
# test <-
# 
# # wb$data[[1]]
# wb %>%
#   mutate(
#     data2 = pmap(
#       list(data,
#            list("cruise_summary"   = cruise_summary, 
#      "chl_progress"     = sample_progress$data[[1]],
#      "hplc_progress"    = sample_progress$data[[2]],
#      "cdom_progress"    = sample_progress$data[[3]],
#      "station_summary"  = mutate(data_summary,
#        sample_collection_time_gmt = as.character(sample_collection_time_gmt)),
#      "sample_meta_data" = mutate(data_unnest,
#                                  sample_collection_time_gmt = as.character(sample_collection_time_gmt))
#      )
#      ),
#      \(x, y) bind_rows(x, y)
#     )
#   ) %>%
#   select(data2)

```



```{r save-inventory}
list("cruise_summary"   = cruise_summary, 
     "chl_progress"     = sample_progress$data[[1]],
     "hplc_progress"    = sample_progress$data[[2]],
     "cdom_progress"    = sample_progress$data[[3]],
     "station_summary"  = data_summary,
     "sample_meta_data" = data_unnest
     ) %>%
  
test  %>%
  pull(data2) %>%
openxlsx::write.xlsx(., 
                     file = here("test_inventory.xlsx"),
                     overwrite = FALSE, 
                     append = TRUE)

shell.exec(here("imars_inventory_chl_hplc_cdom.xlsx"))
```



```{r label}
# # chl_2019 <- 
#    data_unnest %>%
#       # group_by(cruise_id, station, depth_m) %>%
#     select(-identifier, -sample_number, -notes, -c(1:base), -collector) %>%
# 
#     pivot_wider(
#     data         = .,
#     # id_cols      = c(), # *optional* vector of unaffected columns,
#     names_from   = c(sample_type), # category column(s) to pivot wide
#     values_from  = c(vol_ml), # value column(s) that hold data for each category column
#     names_sep    = "_",
#     names_repair = janitor::make_clean_names
#     ) %>%
#       View()
#     
# 
# 
# map2(.x = data_unnest$data,
#      .y = data_unnest$year, 
#      .f = ~ View(.x, title = .y))
# map2(.x = files$data,
#      .y = files$cruise_id, 
#      .f = ~ View(.x, title = .y))
# map(data_unnest$data, ~ unique(.x$cruise_id)  %>%
#   print())
# 
# # 
# data_unnest %>%
# 
#   filter(!(is.na(vol_ml) & is.na(lat) & is.na(lon) & is.na(date_mm_dd_yy))) %>%
#   filter((is.na(vol_ml & is.na(date_mm_dd_yy)))) %>%
#   select(-c(1:base)) %>%
#   pivot_wider(
#     data         = .,
#     # id_cols      = c(), # *optional* vector of unaffected columns,
#     names_from   = c(sample_type), # category column(s) to pivot wide
#     values_from  = c(vol_ml), # value column(s) that hold data for each category column
#     names_sep    = "_",
#     names_repair = janitor::make_clean_names
#     )
#   
  # count() %>%
  # ungroup() %>%
  # pivot_wider(
  #   data         = .,
  #   # id_cols      = c(), # *optional* vector of unaffected columns,
  #   names_from   = c(sample_type), # category column(s) to pivot wide
  #   values_from  = c(n), # value column(s) that hold data for each category column
  #   names_sep    = "_",
  #   names_repair = janitor::make_clean_names
  #   )
  # 
  # data_unnest  %>%
  # group_by(cruise_id, station, depth_m) %>%
    # select(-identifier, -sample_number, -notes, -c(1:base), -collector) %>%
 # pivot_wider(
 #    data         = .,
 #    # id_cols      = c(), # *optional* vector of unaffected columns,
 #    names_from   = c(sample_type), # category column(s) to pivot wide
 #    values_from  = c(vol_ml), # value column(s) that hold data for each category column
 #    names_sep    = "_",
 #    names_repair = janitor::make_clean_names,
 #    values_fn    = list(vol_ml = length)
 #    ) %>%
    # select(-c(millipore_e_dna:last_col())) %>%
    # filter(
    #   # if_any(chl_a:cdom, is.na) &
    #   !(if_all(chl_a:cdom, is.na) & 
    #     if_any(c(date_mm_dd_yy, sample_collection_time_gmt), is.na))
    #   ) %>%
    # ungroup() %>%
    #     janitor::get_dupes(cruise_id, station)
    # 
# data_unnest

```



```{r log-section}
# source(here("scripts/log_file_changes.R"))
# startup()
# current_log()
# log4r_info(verbose = FALSE)
# 
# read.delim("../data_change_logfile.txt")
```

