---
title: "Check Metadata"
author: "Sebastian Di Geronimo"
date: "2023-09-21"
format: html
editor: source
---

The logsheet can end up with errors in date or time.
This protocol plots the cruise track so that you can do manual
checking and editing of the logsheet.

# Load Libraries
```{r setup}
if (!nzchar(system.file(package = "librarian"))) 
  install.packages("librarian")

librarian::shelf(
  quiet = TRUE,
  librarian, conflicted, ggplot2, tibble, tidyr, readr, purrr, dplyr, stringr,
  forcats, lubridate, glue, fs, magrittr, here,

  # additional
  # readxl
  openxlsx, ggarchery, ggtext, ggrepel
)

# shelf(conflicted) # may be needed if won't allow loading of certain packages

conflicts_prefer(
  dplyr::filter(),
  dplyr::select()
  )

source(here("scripts", "misc_functions.R"))
source(here("scripts", "map_files_dwnlod.R"))
source(here("scripts", "load_cruise_globals.R"))
# load_cruise_globals(CRUISE_ID)  # CRUISE_ID from .Rprofile

if (!exists("cloud_dir")) source(here(".Rprofile"))

# `NA` values to skip when reading `.xlsx.
na_skip <- c("#N/A", "NA", "Skipped", "skipped", "na", "n/a", "n./a", "n.a",
             "Flow", "*", stringi::stri_dup("-", 1:20))

```

```{r}
# select logsheet
# meta_file <- here( CRUISE_LOGSHEET )
meta_file <- rstudioapi::selectFile(caption = "Select Logsheet")
```


```{r}
meta_dat <- 
  meta_file %>% 
  getSheetNames() %>% 
  str_subset("(?i)field") %>% 
  read.xlsx(
    meta_file,
    sheet = .,
   # detectDates = TRUE
  ) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(vol_ml)) %>%
  mutate(
    .after = sample_collection_time_gmt,
    date_time = date_mm_dd_yy + sample_collection_time_gmt,
    date_time = janitor::excel_numeric_to_date(
      date_time, 
      include_time = TRUE, 
      tz = "GMT"),
    date_mm_dd_yy = as_date(date_time),
    sample_collection_time_gmt = hms::as_hms(date_time)
  ) %T>% print()
```



# Extract Map Data 
```{r}
map_dat <-
  meta_dat %>%
  distinct(pick(date_mm_dd_yy:station)) %>%
  arrange(date_time) %>%
  mutate(
    lon_lag = dplyr::lead(lon),
    lat_lag = dplyr::lead(lat),
    ) %T>% print()
```


# ---- Base Map Data ----

Topography, ETOPO1, 0.0166667 degrees, Global (longitude -180 to 180), (Ice Sheet Surface) from https://coastwatch.pfeg.noaa.gov/erddap/griddap/

Global Self-consistent, Hierarchical, High-resolution Geography Database (GSHHG) from https://www.ngdc.noaa.gov/mgg/shorelines/

```{r base-map}
if (exists("base_plt")) {
  
  cli::cli_alert_info("{.var base_plot} is already loaded!")
  
} else {
  # path to base map files
  map_loc <- here("data", "map_shp")

  # spatial extent
  exnt <- 
    c(
      xmin = -84, # West
      xmax = -80, # East
      ymin = 24,  # South
      ymax = 28.5 # North 
      )
  ## ---- download topography (.nc) and coastline (.shp)
  world_download(
    path_land  = map_loc,
    path_topo  = map_loc,
    extent     = exnt,
    use_suffix = NULL
  )
  
  ## ---- Load Base Maps for Plotting
  # select and read coastline file from GSHHS then crop
  map_obj <-
    load_map_obj(
      .map_coast = map_loc,
      .map_state = NULL,
      .map_bath  = map_loc,
      .map_file  = "etopo1.nc",
      .extent    = exnt
    )

  ## ---- Plot Base Map
  base_plt <- base_map_plot(map_obj$coast_topo, .bathy = NULL, exnt)
  base_plt
}
```


# ---- Plot Locations ----


```{r add-stations, fig.width=8.5, fig.height=11}
set.seed(123)
cruise <- str_extract(meta_file, "(WS|H|WB)\\d{4,5}")
imars_map <-
  base_plt +
  # ---- Cruise ID
  annotate(
    geom        = "richtext",
    x           = -81.5,
    y           = 28,
    label       = glue("Post Cruise Sample Collection:\n
                       {cruise} (IMaRS)"),
    size        = 6,
    fontface    = "bold",
    fill        = NA,
    label.color = NA,
    family      = "serif"
  ) +

  # ---- Sample Locations
  geom_text_repel(
    aes(x = lon, 
        y = lat, 
        label = station),
    data   = map_dat,
    size   = 2.5,
    hjust  = 0,
    family = "serif"
  ) +
  
  geom_arrowsegment(
    data = map_dat,
    aes(x = lon, 
        xend = lon_lag, 
        y = lat, 
        yend = lat_lag,
        color = fct_reorder(format(date_time, "%B %d"), date_time)
        ), 
    arrow_positions = 0.5,
    arrows = arrow(length = unit(0.1, "inches")),
    na.rm = TRUE
    ) +
  geom_point(
    data = map_dat,
    aes(x = lon, 
        y = lat),
  ) +
  labs(color = "Days of Coverage") +
  # scale_color_manual(
  #   name   = "Samples",
  #   values = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#CC79A7")
  #   ) +

  # ---- Zooplankton
  # new_scale_color() +
  # geom_point(
  #   data  = zoo_loc,
  #   aes(
  #     x = lon,
  #     y = lat,
  #     color = labs
  #   ),
  #   shape = 5,
  #   size  = 2
  # ) +
  # labs(color = "Zooplankton Mesh Size") +
  theme(
    legend.position = c(0.2, 0.14),
    legend.text     = element_text(size = 10),
    legend.title    = element_text(size = 15)
  )

imars_map
```
## Save Map
```{r save-map, fig.width=8.5, fig.height=11}
sv_arg <-
  list(
    save_path = here("data", "metadata", "cruise_map")
  ) %>%
  c(
    file_name =
      here(
        "data", "metadata", "cruise_map",
        glue("{cruise}_imars_sample_locations_collected.jpeg")
      )
  )

if (!file_exists(sv_arg$file_name) || FALSE) {
  # create directory if doens't exist
  fs::dir_create(sv_arg$save_path)
  
  cli::cli_alert_info("Creating map!")
  
  # save map
  cowplot::save_plot(
   sv_arg$file_name,
   plot        = imars_map,
   dpi         = 600,
   base_width  = 8.5,
   base_height = 11
  )
  shell.exec(sv_arg$file_name)
} else {
  cli::cli_alert_info("Map already exists, not overwriting!")
}

if (!file_exists(here(dirname(meta_file), basename(sv_arg$file_name)))) {
  file_copy(
        sv_arg$file_name,
        here(dirname(meta_file)),
        overwrite = TRUE
      )
} else {
  cli_alert_info("Map already copied to cruise directory!")
}
rm(sv_arg)
```



# Plot Multiple Cruises
```{r load-prev-meta}
all_path <- 
  here(cloud_dir) %>%
  dir_ls(regexp = "imars_inventory_chl_hplc_cdom") 

all_meta <- 
  all_path %>% 
  getSheetNames() %>% 
  str_subset("sample_meta_data") %>% 
  read.xlsx(
    all_path, 
    sheet = .
  ) %>% 
  mutate(
    date_time = 
      janitor::excel_numeric_to_date(
        date_time, 
        include_time = TRUE, 
        tz = "UTC")
    ) %>% 
  filter(
    date_time <= as_datetime("2019-10-01")
  ) %T>% print()

map_all <- 
  all_meta %>% 
  select(cruise_id, station, date_time, lon, lat) %>% 
  distinct() %>% 
  arrange(date_time) %>% 
  mutate(
    .by = cruise_id,
    lon_lag = dplyr::lead(lon),
    lat_lag = dplyr::lead(lat),
    ) %>% 
  nest(.by = cruise_id) %T>% print()
```

```{r add-stations, fig.width=8.5, fig.height=11}
set.seed(123)
map_dat <-
  map_all %>%
  filter(str_detect(cruise_id, "23011")) %>%
  # slice(1) %>%
  mutate(
    plt = map2(
      data,
      cruise_id,
      \(x, y) {
        base_plt +
          # ---- Sample Locations
          geom_arrowsegment(
            data = x,
            aes(
              x     = lon,
              xend  = lon_lag,
              y     = lat,
              yend  = lat_lag,
              # color = station
              color = fct_reorder(format(date_time, "%B %d"), date_time)
            ),
            arrow_positions = 0.5,
            arrows = arrow(length = unit(0.1, "inches")),
            na.rm  = TRUE
            # show.legend = FALSE
          ) +
          geom_point(
            data = x,
            aes(
              x = lon,
              y = lat
            ),
          ) +
          geom_text_repel(
            aes(
              x = lon,
              y = lat,
              label = station
            ),
            data = x,
            size = 2.5,
            hjust = 0,
            family = "serif"
          ) +
          labs(
            title = y,
            color = "Days of Coverage"
          ) +
          theme(
            legend.position = c(0.2, 0.14),
            legend.text     = element_text(size = 10),
            legend.title    = element_text(size = 15)
          )
      }
    )
  )

map_dat %$% 
  plt
  

```


## All the Text Contained in Numeric Columns

This text will be converted to `NA` when read by `openxlsx::read.xlsx`

To see the process of extracting this text, look to the end

```{r noaa-na-strings}
noaa_bad <- c(
  "NA", "Skip too shallow",
  "97/98", "109/110",
  "113/114", "115/116",
  "117/118", "-",
  " Aboard R/V Savannah", "Lost ",
  "Lost", "Do Not Use (Discrete Chl samples not usable)",
  "Broken Vial", "Missing Vial ",
  "Missed", "Double labeled",
  "Missing", "RE-CAST NOT SAMPLED",
  "Missing ", "NOT COLLECTED",
  "Broken", "Do Not Use (Discrete Chl samples not usable",
  "Sensor bad", "Missing Tube ",
  "Bottles never tripped - recast", "Sensor Drift ",
  "Gain far too high unusable", "Sensor Cut out",
  "Excluded", "Data lost from Acetone Spill",
  "77,78", "79,80",
  "81,82", "Bad analysis",
  "Acidified prior to Fo", "173/174",
  "175/176", "177/178","179/180", "182/183","184/185", "186/187","188/189", 
  "190/191","192/193", "107/108","112/113", "152/153","155/156", "162/163",
  "165/166", "193/194","128, 129", "131, 132","163, 164", "165, 166","167, 168",
  "169, 170","171, 172", "175, 176","158/159", "160/161","164/165", "172/173",
  "174/175", "178/179","180/181", "189/190","7/8", "43/44","51/52", "85/86",
  "89/90", "135/136","138/139", "153/154","170/171", "69,70","71,72", 
  "Unusable", "Spilled"
)
```


```{r}
noaa_data <- 
  here(cloud_dir) %>%
  dir_ls(regexp = "WSMasterSampleLog") %>%
  str_subset("~", negate = TRUE)

noaa <-
  openxlsx::getSheetNames(noaa_data) %>% 
  str_subset("(?i)all") %>% 
  openxlsx::read.xlsx(
    noaa_data, 
    sheet       = .,
    detectDates = TRUE,
    na.strings  = noaa_bad
  ) %>% 
  janitor::clean_names(replace = c(janitor:::mu_to_u, "#" = "number")) %>% 
  rename(
    "lon" = longitude_decimal,
    "lat" = latitude_decimal) %>%
  mutate(
    .after    = date_gmt,
    time_gmt  = as.numeric(time_gmt),
    time_gmt  = time_gmt + 1,
    time_gmt  = janitor::excel_numeric_to_date(time_gmt, include_time = TRUE),
    time_gmt  = hms::as_hms(time_gmt),
    date_time = ymd_hms(paste(date_gmt, time_gmt))
  ) %>% 
  # select(cruise, station, date_time, lon, lat) %>%
  mutate(
    .by = cruise,
    .after  = time_gmt,
    lon_lag = dplyr::lead(lon),
    lat_lag = dplyr::lead(lat)
    ) %>% 
  distinct()  %T>% print() %>% 
  nest(.by = cruise, .key = "noaa_dat") %T>% print()

# date_time, longitude_decimal, latitude_decimal

```

## Multi Here
```{r, fig.width=15, fig.asp=1.2}
left_join(
  map_all,
  noaa,
  by = c("cruise_id" = "cruise")
) %>% 
  # filter(str_detect(cruise_id, "(WS|H|WB)(19|20|21|22|23)")) %>% 
  # filter(!str_detect(cruise_id, "WS20230|19028")) %>% 
  # slice(3) %>%
  mutate(
    
    plt = pmap(
      list(data, cruise_id, noaa_dat),
      \(x, y, z) {
        
  base_plt +
  # ---- Sample Locations
  geom_arrowsegment(
    data = x,
    aes(
      x     = lon,
      xend  = lon_lag,
      y     = lat,
      yend  = lat_lag,
      # color = station
      color = fct_reorder(format(date_time, "%B %d"), date_time)
        ),
    arrow_positions = 0.5,
    arrows = arrow(length = unit(0.1, "inches")),
    na.rm = TRUE
    # show.legend = FALSE
    ) +
    {if (!is.null(z)) {
      geom_arrowsegment(
    data = z,
    aes(x = lon,
        xend = lon_lag,
        y = lat,
        yend = lat_lag,
        # color = station
        # color = fct_reorder(format(date_time, "%B %d"), date_time)
        ),
    color = "red",
    linetype = "dashed",
    arrow_positions = 0.5,
    arrows = arrow(length = unit(0.1, "inches")),
    na.rm = TRUE
    # show.legend = FALSE
          )
            } } +
  
  geom_point(
    data = x,
    aes(x = lon, 
        y = lat),
  ) +
  geom_text_repel(
    aes(x = lon, 
        y = lat, 
        label = station),
    data   = x,
    size   = 2.5,
    hjust  = 0,
    family = "serif"
  ) +
  labs(
    title = y,
    color = "Days of Coverage"
    ) +
  coord_sf(
    xlim = range(x$lon) + c(-0.5, 0.5),
    ylim = range(x$lat) + c(-0.5, 0.5)
  ) +
  theme(
    legend.position = c(0.2, 0.14),
    legend.text     = element_text(size = 10),
    legend.title    = element_text(size = 15)
  )
      }         
               )
    
  
  ) %$% 
  plt

```




 ------------------------------------------------------------------------------
 ------------------------------------------------------------------------------
 ------------------------------------------------------------------------------
# ---- Find All Text in Columns that Need to be Numeric

Steps:
1. Load NOAA data
2. Clean names
3. Rename lon/lat
4. Create time and datetime
5. Replace is possible to be numeric to "" for the columns of interest
6. Pivot and extract the unique text
7. Paste as a vector 
8. Use as vector in `openxlsx::read.xlsx(na.strings = <vector of text>)` to 
   convert text to `NA`

```{r}
find_text_where_numeric <-
  openxlsx::getSheetNames(noaa_data) %>% 
  str_subset("(?i)all") %>% 
  openxlsx::read.xlsx(
    noaa_data, 
    sheet = .,
    detectDates = TRUE,
    # na.strings = noaa_bad
    # na.strings = c("NA", ignore, str_remove(ignore, "/"), )
  ) %>% 
  janitor::clean_names(replace = c(janitor:::mu_to_u, "#" = "number")) %T>% print() %>% 
  # filter(year(date_gmt) >= 2019) %>% 
  rename("lon" = longitude_decimal,
         "lat" = latitude_decimal) %>%
  mutate(
    .after  = date_gmt,
    # .keep = "used",
    time_gmt = as.numeric(time_gmt),
    time_gmt = time_gmt + 1,
    time_gmt = janitor::excel_numeric_to_date(time_gmt, include_time = TRUE),
    time_gmt = hms::as_hms(time_gmt),
    date_time = ymd_hms(paste(date_gmt, time_gmt))
  ) %>%
  mutate(
    across(
      c(temperature, salinity, depth, contains("avg_chl"):si_u_m, 
        oxygen_mg_l_from_ctd_logs,
        seapoint_chl_fluorometer_voltage:last_col()),
      \(x) if_else(is.na(as.numeric(x)), as.character(x), "")
    )
  )  %>%
        pivot_longer(
     data      = .,
     cols      = c(temperature, salinity, depth, contains("avg_chl"):si_u_m, 
        oxygen_mg_l_from_ctd_logs,
        seapoint_chl_fluorometer_voltage:last_col()),     # columns to pivot long,
     names_to  = "name",  # desired name for category column
     values_to = "value", # desired name for value column
     ) %>%
    distinct(value) %>% 
   arrange(value) %>%
    pull(value) %T>% print() 

noaa_bad <- c("NA",   "Skip too shallow"  ,
              
"97/98"                            ,            "109/110"  ,                                   
"113/114"                          ,            "115/116",
"117/118"                          ,            "-",
" Aboard R/V Savannah"             ,            "Lost ",
"Lost",                                         "Do Not Use (Discrete Chl samples not usable)",
"Broken Vial"                      ,            "Missing Vial ",
"Missed"                           ,            "Double labeled",
"Missing"                          ,            "RE-CAST NOT SAMPLED",
"Missing "                         ,            "NOT COLLECTED",
"Broken"                           ,            "Do Not Use (Discrete Chl samples not usable",
"Sensor bad"                       ,            "Missing Tube ",
"Bottles never tripped - recast"   ,            "Sensor Drift ",
"Gain far too high unusable"       ,            "Sensor Cut out",
"Excluded"                         ,            "Data lost from Acetone Spill",
"77,78"                            ,            "79,80",
"81,82"                            ,            "Bad analysis",
"Acidified prior to Fo"            ,            "173/174",
"175/176"                          ,            "177/178",
"179/180"                          ,            "182/183",
"184/185"                          ,            "186/187",
"188/189"                          ,            "190/191",
"192/193"                          ,            "107/108",
"112/113"                          ,            "152/153",
"155/156"                          ,            "162/163",
"165/166"                          ,            "193/194",
"128, 129"                         ,            "131, 132",
"163, 164"                         ,            "165, 166",
"167, 168"                         ,            "169, 170",
"171, 172"                         ,            "175, 176",
"158/159"                          ,            "160/161",
"164/165"                          ,            "172/173",
"174/175"                          ,            "178/179",
"180/181"                          ,            "189/190",
"7/8"                              ,            "43/44",
"51/52"                            ,            "85/86",
"89/90"                            ,            "135/136",
"138/139"                          ,            "153/154",
"170/171"                          ,            "69,70",
"71,72"  , "Unusable", "Spilled"       )

```

